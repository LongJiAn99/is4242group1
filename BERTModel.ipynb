{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "import string\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('email_training_data.json') as file:\n",
    "    training_data_json = json.load(file)\n",
    "\n",
    "with open('email_testing_data.json') as file:\n",
    "    testing_data_json = json.load(file)\n",
    "\n",
    "with open('emaildata.json') as file:\n",
    "    data = json.load(file)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (525981638.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/4p/ypq_v2317kl8pt0tcn0__fnw0000gn/T/ipykernel_65863/525981638.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    including distribution of labels - visualisation\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# # to do exploratory analysis\n",
    "# including distribution of labels - visualisation\n",
    "# to do correlation matrix of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sender Address    object\n",
       "Sender Name       object\n",
       "Subject           object\n",
       "Content           object\n",
       "Category          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails_content = []\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "label_mapping = {'Personal': 0, 'Marketing': 1, 'Updates': 2}\n",
    "labels = [label_mapping[data['Category']] for data in training_data_json]\n",
    "for data in training_data_json:\n",
    "  emails_content.append(data['Content'])\n",
    "tokenized_emails = [tokenizer(email, padding=\"max_length\", truncation=True, return_tensors=\"pt\") for email in emails_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.stack([email['input_ids'][0] for email in tokenized_emails])\n",
    "attention_masks = torch.stack([email['attention_mask'][0] for email in tokenized_emails])\n",
    "labels_tensor = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Convert labels to tensor and move to the device\n",
    "labels_tensor = torch.tensor(labels).to(device)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 2\n",
    "\n",
    "# Create DataLoader\n",
    "data = torch.utils.data.TensorDataset(input_ids, attention_masks, labels_tensor)\n",
    "data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# Train the model\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        input_ids_batch, attention_masks_batch, labels_batch = [t.to(device) for t in batch]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids_batch, attention_mask=attention_masks_batch, labels=labels_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update weights every 16 batches\n",
    "        if (i + 1) % 16 == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Free up memory\n",
    "        del input_ids_batch, attention_masks_batch, labels_batch, outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Print average loss for each epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(data_loader)}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'bert_email_classification_model1.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4p/ypq_v2317kl8pt0tcn0__fnw0000gn/T/ipykernel_65863/1321011555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Move the model to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Move the model to the device\n",
    "# model.to(device)\n",
    "\n",
    "# # Define optimizer and loss function\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# # Convert labels to tensor and move to the device\n",
    "# labels_tensor = torch.tensor(labels).to(device)\n",
    "\n",
    "# # Define batch size\n",
    "# batch_size = 2  # Reduce batch size\n",
    "\n",
    "# # Create DataLoader\n",
    "# data = torch.utils.data.TensorDataset(input_ids, attention_masks, labels_tensor)\n",
    "# data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "\n",
    "# # Define number of epochs\n",
    "# num_epochs = 2  # Adjust as needed\n",
    "\n",
    "# # Train the model\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for batch in data_loader:\n",
    "#         batch = tuple(t.to(device) for t in batch)\n",
    "#         input_ids_batch, attention_masks_batch, labels_batch = batch\n",
    "\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(input_ids_batch, attention_mask=attention_masks_batch, labels=labels_batch)\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = outputs.loss\n",
    "        \n",
    "#         # Backward pass and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#         # Free up memory\n",
    "#         del input_ids_batch, attention_masks_batch, labels_batch, outputs, loss\n",
    "#         torch.cuda.empty_cache()\n",
    "\n",
    "#     # Print average loss for each epoch\n",
    "#     print(f\"Epoch {epoch+1}, Loss: {running_loss / len(data_loader)}\")\n",
    "\n",
    "# # Save the trained model\n",
    "# torch.save(model.state_dict(), 'bert_email_classification_model1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "model.load_state_dict(torch.load('bert_email_classification_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define batch size for testing\n",
    "test_batch_size = 32  # Adjust as needed\n",
    "\n",
    "# Define function to predict labels for a batch of input texts\n",
    "def predict_labels(texts):\n",
    "    # Tokenize input texts\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_masks = inputs['attention_mask'].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():  # Disable gradient tracking during inference\n",
    "        outputs = model(input_ids, attention_mask=attention_masks)\n",
    "    \n",
    "    # Get predicted labels\n",
    "    predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "    return predicted_labels.cpu().tolist()\n",
    "\n",
    "# Assuming you have a list of test emails stored in 'test_emails'\n",
    "# Replace 'test_emails' with your actual test data\n",
    "test_predicted_labels = []\n",
    "test_emails_content = []\n",
    "for data in testing_data_json:\n",
    "  test_emails_content.append(data['content'])\n",
    "\n",
    "# Iterate over test data in batches and predict labels\n",
    "for i in range(0, len(test_emails_content), test_batch_size):\n",
    "    batch_texts = test_emails_content[i:i+test_batch_size]\n",
    "    batch_predicted_labels = predict_labels(batch_texts)\n",
    "    test_predicted_labels.extend(batch_predicted_labels)\n",
    "\n",
    "# Now, 'test_predicted_labels' contains the predicted labels for the test emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_predicted_labels)\n",
    "label_mapping = {'Personal': 0, 'Marketing': 1, 'Updates': 2}\n",
    "correct_test_labels = [label_mapping[data['Category']] for data in testing_data_json]\n",
    "print(correct_test_labels)\n",
    "\n",
    "counter = 0\n",
    "for i in range(len(test_predicted_labels)):\n",
    "    if test_predicted_labels[i] == correct_test_labels[i]:\n",
    "        counter += 1\n",
    "print(f\"Accuracy: {counter / len(test_predicted_labels) * 100}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
